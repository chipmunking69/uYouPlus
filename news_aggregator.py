#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–ü—Ä–æ–≥—Ä–∞–º–º–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ –∫–æ–º–ø–∞–Ω–∏—è–º
"""

import requests
import json
import time
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import re
from urllib.parse import quote_plus
import feedparser
from bs4 import BeautifulSoup
import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class NewsAggregator:
    """–ö–ª–∞—Å—Å –¥–ª—è –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ –∫–æ–º–ø–∞–Ω–∏—è–º"""
    
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)
    
    def search_google_news(self, company_name: str, days_back: int = 7) -> List[Dict]:
        """–ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ Google News"""
        news_items = []
        try:
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –¥–ª—è Google News RSS
            query = quote_plus(f'"{company_name}" OR "{company_name}" –Ω–æ–≤–æ—Å—Ç–∏')
            rss_url = f"https://news.google.com/rss/search?q={query}&hl=ru&gl=RU&ceid=RU:ru"
            
            logger.info(f"–ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è –∫–æ–º–ø–∞–Ω–∏–∏: {company_name}")
            
            # –ü–∞—Ä—Å–∏–º RSS —Ñ–∏–¥
            feed = feedparser.parse(rss_url)
            
            cutoff_date = datetime.now() - timedelta(days=days_back)
            
            for entry in feed.entries[:20]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 20 –Ω–æ–≤–æ—Å—Ç–µ–π
                try:
                    # –ü–∞—Ä—Å–∏–º –¥–∞—Ç—É –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
                    pub_date = datetime(*entry.published_parsed[:6]) if hasattr(entry, 'published_parsed') and entry.published_parsed else datetime.now()
                    
                    if pub_date >= cutoff_date:
                        news_item = {
                            'title': entry.title,
                            'link': entry.link,
                            'published': pub_date.strftime('%Y-%m-%d %H:%M'),
                            'source': 'Google News',
                            'description': getattr(entry, 'summary', ''),
                        }
                        news_items.append(news_item)
                except Exception as e:
                    logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –Ω–æ–≤–æ—Å—Ç–∏: {e}")
                    continue
                    
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –≤ Google News: {e}")
            
        return news_items
    
    def search_yandex_news(self, company_name: str) -> List[Dict]:
        """–ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ –Ø–Ω–¥–µ–∫—Å –ù–æ–≤–æ—Å—Ç–∏ (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç)"""
        news_items = []
        try:
            # –ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            query = f"{company_name} –Ω–æ–≤–æ—Å—Ç–∏"
            search_url = f"https://yandex.ru/news/search?text={quote_plus(query)}"
            
            response = self.session.get(search_url, timeout=10)
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # –ò—â–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π (–ø—Ä–∏–º–µ—Ä–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞)
                news_elements = soup.find_all(['h3', 'h2'], class_=re.compile(r'.*title.*|.*headline.*'))
                
                for element in news_elements[:10]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
                    link_elem = element.find('a')
                    if link_elem:
                        title = element.get_text(strip=True)
                        link = link_elem.get('href', '')
                        
                        if company_name.lower() in title.lower():
                            news_item = {
                                'title': title,
                                'link': link,
                                'published': datetime.now().strftime('%Y-%m-%d %H:%M'),
                                'source': '–Ø–Ω–¥–µ–∫—Å –ù–æ–≤–æ—Å—Ç–∏',
                                'description': '',
                            }
                            news_items.append(news_item)
                            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –≤ –Ø–Ω–¥–µ–∫—Å –ù–æ–≤–æ—Å—Ç—è—Ö: {e}")
            
        return news_items
    
    def get_article_content(self, url: str) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Å—Ç–∞—Ç—å–∏ –ø–æ URL"""
        try:
            response = self.session.get(url, timeout=10)
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # –£–¥–∞–ª—è–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
                for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):
                    element.decompose()
                
                # –ò—â–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
                content_selectors = [
                    'article', '.article-content', '.post-content', 
                    '.content', '.entry-content', 'main', '.main-content'
                ]
                
                content = ""
                for selector in content_selectors:
                    content_elem = soup.select_one(selector)
                    if content_elem:
                        content = content_elem.get_text(separator=' ', strip=True)
                        break
                
                if not content:
                    # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –ø–æ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º, –±–µ—Ä–µ–º –≤—Å–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
                    paragraphs = soup.find_all('p')
                    content = ' '.join([p.get_text(strip=True) for p in paragraphs])
                
                return content[:2000]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä
                
        except Exception as e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç–∞—Ç—å–∏ {url}: {e}")
            
        return ""
    
    def summarize_news(self, news_items: List[Dict], company_name: str) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Å–≤–æ–¥–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π"""
        if not news_items:
            return f"–ù–æ–≤–æ—Å—Ç–∏ –ø–æ –∫–æ–º–ø–∞–Ω–∏–∏ '{company_name}' –Ω–µ –Ω–∞–π–¥–µ–Ω—ã."
        
        summary = f"\nüè¢ –°–í–û–î–ö–ê –ù–û–í–û–°–¢–ï–ô –ü–û –ö–û–ú–ü–ê–ù–ò–ò: {company_name.upper()}\n"
        summary += "=" * 60 + "\n\n"
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ –¥–Ω—è–º
        news_by_date = {}
        for news in news_items:
            date_key = news['published'].split(' ')[0]
            if date_key not in news_by_date:
                news_by_date[date_key] = []
            news_by_date[date_key].append(news)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ (–Ω–æ–≤—ã–µ —Å–Ω–∞—á–∞–ª–∞)
        sorted_dates = sorted(news_by_date.keys(), reverse=True)
        
        total_news = len(news_items)
        summary += f"üìä –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π: {total_news}\n"
        summary += f"üìÖ –ü–µ—Ä–∏–æ–¥: {sorted_dates[-1] if sorted_dates else 'N/A'} - {sorted_dates[0] if sorted_dates else 'N/A'}\n\n"
        
        # –í—ã–≤–æ–¥–∏–º –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ –¥–∞—Ç–∞–º
        for date in sorted_dates:
            summary += f"üìÖ {date}\n"
            summary += "-" * 20 + "\n"
            
            for i, news in enumerate(news_by_date[date], 1):
                summary += f"{i}. üì∞ {news['title']}\n"
                summary += f"   üîó –ò—Å—Ç–æ—á–Ω–∏–∫: {news['source']}\n"
                if news['description']:
                    desc = news['description'][:200] + "..." if len(news['description']) > 200 else news['description']
                    summary += f"   üìù {desc}\n"
                summary += f"   üåê {news['link']}\n\n"
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫—Ä–∞—Ç–∫–∏–π –∞–Ω–∞–ª–∏–∑
        summary += "\nüìà –ö–†–ê–¢–ö–ò–ô –ê–ù–ê–õ–ò–ó:\n"
        summary += "-" * 20 + "\n"
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≤ –∑–∞–≥–æ–ª–æ–≤–∫–∞—Ö
        all_titles = " ".join([news['title'].lower() for news in news_items])
        
        keywords_analysis = {
            '—Ñ–∏–Ω–∞–Ω—Å—ã': ['–ø—Ä–∏–±—ã–ª—å', '—É–±—ã—Ç–æ–∫', '–¥–æ—Ö–æ–¥—ã', '–≤—ã—Ä—É—á–∫–∞', '–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏', '–∞–∫—Ü–∏–∏', '–∫–∞–ø–∏—Ç–∞–ª'],
            '—Ä–∞–∑–≤–∏—Ç–∏–µ': ['—Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ', '—Ä–æ—Å—Ç', '—Ä–∞–∑–≤–∏—Ç–∏–µ', '–∑–∞–ø—É—Å–∫', '–æ—Ç–∫—Ä—ã—Ç–∏–µ', '–Ω–æ–≤—ã–π'],
            '–ø—Ä–æ–±–ª–µ–º—ã': ['–ø—Ä–æ–±–ª–µ–º—ã', '–∫—Ä–∏–∑–∏—Å', '—É–±—ã—Ç–∫–∏', '—Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ', '–∑–∞–∫—Ä—ã—Ç–∏–µ', '–±–∞–Ω–∫—Ä–æ—Ç—Å—Ç–≤–æ'],
            '–ø–∞—Ä—Ç–Ω–µ—Ä—Å—Ç–≤–æ': ['—Å–¥–µ–ª–∫–∞', '–ø–∞—Ä—Ç–Ω–µ—Ä—Å—Ç–≤–æ', '—Å–æ–≥–ª–∞—à–µ–Ω–∏–µ', '–∫–æ–Ω—Ç—Ä–∞–∫—Ç', '—Å–æ—Ç—Ä—É–¥–Ω–∏—á–µ—Å—Ç–≤–æ']
        }
        
        found_themes = []
        for theme, keywords in keywords_analysis.items():
            if any(keyword in all_titles for keyword in keywords):
                found_themes.append(theme)
        
        if found_themes:
            summary += f"üîç –û—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ–º—ã: {', '.join(found_themes)}\n"
        else:
            summary += "üîç –û—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ–º—ã: –æ–±—â–∏–µ –Ω–æ–≤–æ—Å—Ç–∏\n"
        
        summary += f"‚è∞ –°–≤–æ–¥–∫–∞ —Å–æ–∑–¥–∞–Ω–∞: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
        
        return summary
    
    def search_company_news(self, company_name: str, days_back: int = 7) -> str:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –Ω–æ–≤–æ—Å—Ç–µ–π"""
        logger.info(f"–ù–∞—á–∏–Ω–∞—é –ø–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è –∫–æ–º–ø–∞–Ω–∏–∏: {company_name}")
        
        all_news = []
        
        # –ü–æ–∏—Å–∫ –≤ Google News
        google_news = self.search_google_news(company_name, days_back)
        all_news.extend(google_news)
        logger.info(f"–ù–∞–π–¥–µ–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π –≤ Google News: {len(google_news)}")
        
        # –ü–æ–∏—Å–∫ –≤ –Ø–Ω–¥–µ–∫—Å –ù–æ–≤–æ—Å—Ç—è—Ö
        yandex_news = self.search_yandex_news(company_name)
        all_news.extend(yandex_news)
        logger.info(f"–ù–∞–π–¥–µ–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π –≤ –Ø–Ω–¥–µ–∫—Å –ù–æ–≤–æ—Å—Ç—è—Ö: {len(yandex_news)}")
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º
        unique_news = []
        seen_titles = set()
        for news in all_news:
            title_key = news['title'].lower().strip()
            if title_key not in seen_titles:
                seen_titles.add(title_key)
                unique_news.append(news)
        
        logger.info(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π: {len(unique_news)}")
        
        # –°–æ–∑–¥–∞–µ–º —Å–≤–æ–¥–∫—É
        summary = self.summarize_news(unique_news, company_name)
        
        return summary


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–æ–≥—Ä–∞–º–º—ã"""
    print("üîç –ê–ì–†–ï–ì–ê–¢–û–† –ù–û–í–û–°–¢–ï–ô –ü–û –ö–û–ú–ü–ê–ù–ò–Ø–ú")
    print("=" * 50)
    
    aggregator = NewsAggregator()
    
    while True:
        try:
            company_name = input("\nüìù –í–≤–µ–¥–∏—Ç–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏ (–∏–ª–∏ '–≤—ã—Ö–æ–¥' –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è): ").strip()
            
            if not company_name:
                print("‚ùå –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏.")
                continue
            
            if company_name.lower() in ['–≤—ã—Ö–æ–¥', 'exit', 'quit', 'q']:
                print("üëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
                break
            
            print(f"\n‚è≥ –ò—â—É –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ –∫–æ–º–ø–∞–Ω–∏–∏ '{company_name}'...")
            print("–≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–µ–∫—É–Ω–¥...\n")
            
            # –ü–æ–ª—É—á–∞–µ–º —Å–≤–æ–¥–∫—É –Ω–æ–≤–æ—Å—Ç–µ–π
            summary = aggregator.search_company_news(company_name)
            
            # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            print(summary)
            
            # –ü—Ä–µ–¥–ª–∞–≥–∞–µ–º —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ —Ñ–∞–π–ª
            save_choice = input("\nüíæ –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å–≤–æ–¥–∫—É –≤ —Ñ–∞–π–ª? (y/n): ").strip().lower()
            if save_choice in ['y', 'yes', '–¥–∞', '–¥']:
                filename = f"news_summary_{company_name.replace(' ', '_')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
                try:
                    with open(filename, 'w', encoding='utf-8') as f:
                        f.write(summary)
                    print(f"‚úÖ –°–≤–æ–¥–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ —Ñ–∞–π–ª: {filename}")
                except Exception as e:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞: {e}")
            
        except KeyboardInterrupt:
            print("\n\nüëã –ü—Ä–æ–≥—Ä–∞–º–º–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º.")
            break
        except Exception as e:
            print(f"‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {e}")
            print("–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑ –∏–ª–∏ –≤–≤–µ–¥–∏—Ç–µ '–≤—ã—Ö–æ–¥' –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è.")


if __name__ == "__main__":
    main()