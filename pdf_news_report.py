import base64
import os
import re
import unicodedata
import html
from uuid import uuid4
from datetime import datetime
from typing import Optional

import requests
import fitz  # PyMuPDF

from news_utils import get_news_summary

# ============================ –ù–ê–°–¢–†–û–ô–ö–ò ============================
CLIENT_ID = "395c6aed-f8a0-409d-bc19-e302408bf922"
CLIENT_SECRET = "bc7a96bc-ffe4-431c-a5b1-0a4c39a0c090"
SCOPE = "GIGACHAT_API_PERS"

PDF_PATH = "/Users/chipmunks69/Documents/pdf_downloads/test.pdf"  # –æ–±–Ω–æ–≤–∏—Ç–µ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
OUTPUT_TXT = "report.txt"
OUTPUT_HTML = "report.html"

TOKEN_URL = "https://ngw.devices.sberbank.ru:9443/api/v2/oauth"
GIGACHAT_URL = "https://gigachat.devices.sberbank.ru/api/v1/chat/completions"

# (–Ω–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ, –Ω–æ —É–±–∏—Ä–∞–µ—Ç –≤–∞—Ä–Ω–∏–Ω–≥–∏ –ø—Ä–∏ verify=False)
try:
    import urllib3
    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
except Exception:
    pass

# ===================== –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–û–ï =====================

def slugify(text: str) -> str:
    text = unicodedata.normalize("NFKD", text)
    text = "".join(ch for ch in text if not unicodedata.combining(ch))
    text = re.sub(r"[^\w\s\-\.]", "", text, flags=re.UNICODE)
    text = re.sub(r"\s+", "-", text.strip(), flags=re.UNICODE)
    text = text.strip("-_.").lower()
    return text or f"id-{uuid4().hex[:8]}"


def detect_company_name(text: str) -> Optional[str]:
    """–ü—ã—Ç–∞–µ–º—Å—è –≤—ã—Ç–∞—â–∏—Ç—å –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏ –∏–∑ PDF-—Ç–µ–∫—Å—Ç–∞ –ø–æ –ø—Ä–æ—Å—Ç—ã–º –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º."""
    patterns = [
        r"(?i)(?:–æ–±—â–µ—Å—Ç–≤[–æ–∞] —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å—é|–æ–æ–æ|–ø—É–±–ª–∏—á–Ω–æ–µ –∞–∫—Ü–∏–æ–Ω–µ—Ä–Ω–æ–µ –æ–±—â–µ—Å—Ç–≤–æ|–ø–∞–æ|–∑–∞–∫—Ä—ã—Ç–æ–µ –∞–∫—Ü–∏–æ–Ω–µ—Ä–Ω–æ–µ –æ–±—â–µ—Å—Ç–≤–æ|–∑–∞–æ|–∞–∫—Ü–∏–æ–Ω–µ—Ä–Ω–æ–µ –æ–±—â–µ—Å—Ç–≤–æ|–∞–æ)\s+\"?([A-Za-z–ê-–Ø–∞-—è0-9 ][^\n\"]{2,})\"?",
        r"(?i)–ü–æ–ª–Ω–æ–µ\s+–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ[^\n]{0,50}[\n:]+\s*\"?(.{3,120}?)\"?\s*(?:\n|$)",
    ]
    for p in patterns:
        m = re.search(p, text)
        if m:
            return m.group(1).strip()
    # fallback: –ø–µ—Ä–≤—ã–µ 5 —Å–ª–æ–≤ –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–æ–∫–∏
    first_line = text.split("\n", 1)[0]
    return " ".join(first_line.split()[:5]) if first_line else None


def get_access_token():
    creds_b64 = base64.b64encode(f"{CLIENT_ID}:{CLIENT_SECRET}".encode()).decode()

    headers = {
        "Authorization": f"Basic {creds_b64}",
        "RqUID": str(uuid4()),
        "Content-Type": "application/x-www-form-urlencoded",
    }
    data = {"scope": SCOPE}

    resp = requests.post(TOKEN_URL, headers=headers, data=data, verify=False, timeout=30)
    resp.raise_for_status()
    return resp.json()["access_token"]


def extract_text(pdf_path: str) -> str:
    doc = fitz.open(pdf_path)
    return "\n".join(page.get_text() for page in doc)


def analyze_company(text: str, news_summary: str, news_articles: list[tuple[str, str]], token: str) -> str:
    """–§–æ—Ä–º–∏—Ä—É–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç —Å –Ω–æ–≤–æ—Å—Ç—è–º–∏ –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –≤ GigaChat."""
    headers = {
        "Authorization": f"Bearer {token}",
        "Content-Type": "application/json",
    }

    news_block = ""
    if news_summary:
        news_block += "\n\n–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ (–Ω–æ–≤–æ—Å—Ç–∏):\n"
        news_block += f"–°–≤–æ–¥–∫–∞ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π: {news_summary}\n\n"
        if news_articles:
            news_block += "–°–ø–∏—Å–æ–∫ —Å—Ç–∞—Ç–µ–π:\n" + "\n".join(
                f"- {t} ({l})" for t, l in news_articles[:10]
            )
            news_block += "\n\n–ò—Å–ø–æ–ª—å–∑—É–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ñ–∞–∫—Ç–æ–≤, –ø–æ–∏—Å–∫–∞ –Ω–æ–≤—ã—Ö —Å–≤—è–∑–µ–π –∏ –≤–æ–∑–º–æ–∂–Ω—ã—Ö –±–µ–Ω–µ—Ñ–∏—Ü–∏–∞—Ä–æ–≤."

    prompt = (
        "–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–µ –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–µ–ª–æ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. "
        "–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π PDF-—Ñ–∞–π–ª —Å –ø–æ–¥—Ä–æ–±–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –∫–æ–º–ø–∞–Ω–∏–∏ –∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ –æ –Ω–µ–π. "
        "–°–∫–æ–º–±–∏–Ω–∏—Ä—É–π –¥–∞–Ω–Ω—ã–µ, —á—Ç–æ–±—ã –ø–æ—Å—Ç—Ä–æ–∏—Ç—å —Ü–µ–ª—å–Ω—ã–π –æ—Ç—á—ë—Ç. "
        "–¢–≤–æ—è –∑–∞–¥–∞—á–∞: \n"
        "1. –°–æ—Å—Ç–∞–≤—å –ø–æ–¥—Ä–æ–±–Ω–æ–µ –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–æ–µ summary —Å –∞–∫—Ü–µ–Ω—Ç–æ–º –Ω–∞ —Ä–∞–∑–¥–µ–ª—ã, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–º–æ–≥–∞—é—Ç –≤—ã—è–≤–∏—Ç—å –±–µ–Ω–µ—Ñ–∏—Ü–∏–∞—Ä–æ–≤ (—Å–º. –Ω–∏–∂–µ). "
        "2. –ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–π –≤ –æ—Ç—á—ë—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ–∞–∫—Ç—ã –∏–∑ –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤. –ï—Å–ª–∏ –Ω–æ–≤–æ—Å—Ç–∏ —Å–æ–¥–µ—Ä–∂–∞—Ç —Å–≤–µ–¥–µ–Ω–∏—è –æ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–∏–∫–∞—Ö, —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–µ, —Å—É–¥–µ–±–Ω—ã—Ö –ø—Ä–æ—Ü–µ—Å—Å–∞—Ö, –≥–æ—Å–∫–æ–Ω—Ç—Ä–∞–∫—Ç–∞—Ö –∏ –ø—Ä. ‚Äî –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –ø—Ä–∏–≤–µ–¥–∏ –∏—Ö —Å —É–∫–∞–∑–∞–Ω–∏–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞. "
        "3. –°–ø–∏—Å–æ–∫ –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º—ã—Ö –±–µ–Ω–µ—Ñ–∏—Ü–∏–∞—Ä–æ–≤ (—Ç–æ–ª—å–∫–æ —Ñ–∏–∑–∏—á–µ—Å–∫–∏–µ –ª–∏—Ü–∞) –Ω—É–∂–Ω–æ —Ä–∞—Å—à–∏—Ä–∏—Ç—å, —É—á–∏—Ç—ã–≤–∞—è –∫–∞–∫ PDF, —Ç–∞–∫ –∏ —Å–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏. –î–ª—è –∫–∞–∂–¥–æ–≥–æ —É–∫–∞–∂–∏ —Ä–æ–ª—å –∏ –∞—Ä–≥—É–º–µ–Ω—Ç–∞—Ü–∏—é. "
        "4. –í–µ—Ä–Ω–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –≤–∏–¥–µ HTML-–¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å –º–∏–Ω–∏–º–∞–ª–∏—Å—Ç–∏—á–Ω—ã–º —Å—Ç–∏–ª–µ–º –∏ –Ω–∞–≤–∏–≥–∞—Ü–∏–µ–π –ø–æ —Ä–∞–∑–¥–µ–ª–∞–º (–∏—Å–ø–æ–ª—å–∑—É–π —Ç–µ –∂–µ –ø—Ä–∞–≤–∏–ª–∞ —Ä–∞–∑–º–µ—Ç–∫–∏, —á—Ç–æ –∏ —Ä–∞–Ω–µ–µ). "
        "\n\n–î–∞–Ω–Ω—ã–µ –∏–∑ PDF:\n" + text + news_block
    )

    payload = {
        "model": "GigaChat",
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0.2,
        "max_tokens": 2000,
    }

    resp = requests.post(GIGACHAT_URL, headers=headers, json=payload, verify=False, timeout=120)
    resp.raise_for_status()
    return resp.json()["choices"][0]["message"]["content"]


# ===================== –†–ê–ó–ú–ï–¢–ö–ê –¢–ï–ö–°–¢–ê -> HTML =====================
# (–∏—Å–ø–æ–ª—å–∑—É–µ–º —Ñ—É–Ω–∫—Ü–∏–∏ –∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Å–∫—Ä–∏–ø—Ç–∞ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)
# –ß—Ç–æ–±—ã –Ω–µ –∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å 350+ —Å—Ç—Ä–æ–∫, –∏–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å

from types import ModuleType

# –ø–æ–ø—ã—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å utils –∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏, fallback ‚Äî –ª–æ–∫–∞–ª—å–Ω–∞—è –∫–æ–ø–∏—è –Ω–∏–∂–µ
try:
    from report_html_utils import clean_text, parse_to_sections, build_nav  # type: ignore
except ImportError:

    # --- –º–∏–Ω–∏-–∫–æ–ø–∏—è –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π (—É–ø—Ä–æ—â—ë–Ω–Ω–∞—è) -------------------------
    def clean_text(plain_text: str) -> str:
        t = plain_text or ""
        t = re.sub(r"^\s*#{1,6}\s*", "", t, flags=re.MULTILINE)
        t = re.sub(r"\*\*(.*?)\*\*", r"\1", t, flags=re.DOTALL)
        t = re.sub(r"`([^`]+)`", r"\1", t)
        t = re.sub(r"\n{3,}", "\n\n", t)
        return t.strip()

    def parse_to_sections(text: str):
        # —Ç—Ä–∏–≤–∏–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥: –∫–∞–∂–¥–∞—è —Å—Ç—Ä–æ–∫–∞ —Å '##' ‚Äî –Ω–æ–≤—ã–π —Ä–∞–∑–¥–µ–ª
        sections = []
        current = {"title": "–û—Ç—á—ë—Ç", "level": 1, "id": slugify("–û—Ç—á—ë—Ç"), "blocks": []}
        for line in text.splitlines():
            if line.startswith("## "):
                sections.append(current)
                current = {
                    "title": line[3:],
                    "level": 2,
                    "id": slugify(line[3:]),
                    "blocks": [],
                }
            else:
                current["blocks"].append(f"<p>{html.escape(line)}</p>")
        sections.append(current)
        return sections

    def build_nav(sections):
        return "<ul class='toc'>" + "".join(
            f"<li><a href='#{s['id']}'>{html.escape(s['title'])}</a></li>" for s in sections
        ) + "</ul>"


def build_html_report(plain_text: str) -> str:
    text = clean_text(plain_text)
    sections = parse_to_sections(text)

    body_html = "".join(
        f"<section id='{s['id']}'><h2>{html.escape(s['title'])}</h2>{''.join(s['blocks'])}</section>" for s in sections
    )
    nav_html = build_nav(sections)
    gen_date = datetime.now().strftime("%d.%m.%Y %H:%M")
    return f"""
<!doctype html><html lang='ru'><head><meta charset='utf-8'/><title>–ê–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–π –æ—Ç—á—ë—Ç</title></head>
<body><aside>{nav_html}</aside><main><h1>–ê–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–π –æ—Ç—á—ë—Ç</h1><p><em>–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {gen_date}</em></p>{body_html}</main></body></html>
"""


# ===================== –¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞ =====================

def main():
    if not os.path.exists(PDF_PATH):
        print(f"‚ùå –§–∞–π–ª {PDF_PATH} –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return

    print("üîë –ü–æ–ª—É—á–∞—é —Ç–æ–∫–µ–Ω...")
    token = get_access_token()

    print("üìÑ –ò–∑–≤–ª–µ–∫–∞—é —Ç–µ–∫—Å—Ç –∏–∑ PDF...")
    pdf_text = extract_text(PDF_PATH)

    print("üè∑Ô∏è  –û–ø—Ä–µ–¥–µ–ª—è—é –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏...")
    company_name = detect_company_name(pdf_text) or "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –∫–æ–º–ø–∞–Ω–∏—è"
    print(f"   ‚Üí –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ: {company_name}")

    print("üì∞ –ó–∞–≥—Ä—É–∂–∞—é –Ω–æ–≤–æ—Å—Ç–∏ –æ –∫–æ–º–ø–∞–Ω–∏–∏...")
    news_summary, news_articles = get_news_summary(company_name, max_results=20, summary_sentences=5)

    print("ü§ñ –û—Ç–ø—Ä–∞–≤–ª—è—é –≤—Å—ë –Ω–∞ –∞–Ω–∞–ª–∏–∑ –≤ GigaChat...")
    plain_text = analyze_company(pdf_text, news_summary, news_articles, token)

    print("üíæ –°–æ—Ö—Ä–∞–Ω—è—é —Å—ã—Ä–æ–π —Ç–µ–∫—Å—Ç –æ—Ç—á—ë—Ç–∞...")
    with open(OUTPUT_TXT, "w", encoding="utf-8") as f:
        f.write(plain_text)

    print("üé® –ì–µ–Ω–µ—Ä–∏—Ä—É—é HTML-–æ—Ç—á—ë—Ç...")
    html_content = build_html_report(plain_text)
    with open(OUTPUT_HTML, "w", encoding="utf-8") as f:
        f.write(html_content)

    print(f"‚úÖ –ì–æ—Ç–æ–≤–æ! HTML —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {OUTPUT_HTML}")


if __name__ == "__main__":
    main()